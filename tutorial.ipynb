{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tutorial.ipynb","provenance":[{"file_id":"1bRfdfYN9FmO4D1FUciDBBrns1L0IhWMT","timestamp":1584373058957}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LFTA-BmvAz6x","colab_type":"text"},"source":["# Colab NLP2 Tutorial\n","\n","This notebook contains a short tutorial that shows basic Google Colab usage,\n"," and is a Pytorch NLP refresher.\n","\n"," You will train a Sentiment Analysis model on the Amazon Fine Food Review dataset, and evaluate your models in Tensorboard."]},{"cell_type":"markdown","metadata":{"id":"yiKis7QBBfnD","colab_type":"text"},"source":["**Step 1**\n","\n","In order to load code and data to your notebook, you need get access to your Google Drive. Mount your drive, and change the directory to your drive folder with the colab tutorial files."]},{"cell_type":"code","metadata":{"id":"atTj0LMQHQnK","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","# You can change your current path with notebook Magic commands,\n","# and execute shell commands by adding a exclamation mark to the start.\n","# For example:\n","\n","# %cd \"/content/drive/My Drive/NLP2/colab tutorial\"\n","# !ls\n","\n","# Here you should see the training files and tokenizers.py"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zUHMa5ikCLsr","colab_type":"text"},"source":["**Step 2**\n","\n","In this tutorial we use a modified version of the Amazon Fine Food Review (AFFR) dataset. Download the dataset from Canvas, put it on your Google Drive and load it here.\n","\n","The dataset has two columns:\n","\n","\n","*   a `Review` column, that contains the review of a product\n","*   a `Score` column, which is `0` for a bad review, `1` for a neutral review, and `2` for a positive review.\n"]},{"cell_type":"code","metadata":{"id":"UN-9NXrqkmQT","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","train_path = 'affr_train.csv'\n","valid_path = 'affr_valid.csv'\n","\n","# Load train data\n","data_train = pd.read_csv(train_path)\n","reviews_train = data_train.Review.tolist()\n","scores_train = data_train.Score.tolist()\n","\n","# Load valid data\n","data_valid = pd.read_csv(valid_path)\n","reviews_valid = data_valid.Review.tolist()\n","scores_valid = data_valid.Score.tolist()\n","\n","# Visualize the first few datapoints\n","data_train.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5kyrof7YDQIJ","colab_type":"text"},"source":["**Step 3**\n","\n","On Canvas you will also find ```tokenizers.py```, which contains a simple tokenizer. We train this tokenizer on the reviews in the train set, and limit the maximum vocabulary size to 10.000 to keep the model reasonably fast."]},{"cell_type":"code","metadata":{"id":"W1C7puHF44wI","colab_type":"code","colab":{}},"source":["from tokenizers import WordTokenizer\n","\n","# Train your tokenizer.\n","tokenizer = WordTokenizer(reviews_train, max_vocab_size=10000)\n","\n","# Check if everything works.\n","for sentence in reviews_train[:5]:\n","    tokenized = tokenizer.encode(sentence, add_special_tokens=False)\n","    sentence_decoded = tokenizer.decode(tokenized)\n","    print('original:', sentence)\n","    print('tokenized:', tokenized)\n","    print('decoded:', sentence_decoded)\n","    print()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nm-G-vWrEfz-","colab_type":"text"},"source":["**Step 4a**\n","\n","Pytorch has built-in classes that handle all data loading functionality. We will write a ```torch.utils.data.Dataset```, that handles preprocessing and tokenization, and pass it to a ```torch.utils.data.DataLoader``` which handles shuffling, padding and batching.\n","\n","To write a `Dataset` class you need two things:\n","\n","\n","*   A ```__len__``` method, that returns the lengths of your Dataset.\n","*   A ```__getitem__``` method, that returns an instance from your dataset at the given index."]},{"cell_type":"code","metadata":{"id":"vBxlCqToNrNq","colab_type":"code","colab":{}},"source":["from torch.utils.data import Dataset\n","\n","class AFFRDataset(Dataset):\n","    def __init__(self, sentences, labels, tokenizer):\n","        self.sentences = sentences\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        \"\"\"Returns the number of items in the dataset\"\"\"\n","        return len(self.sentences)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns the datapoint at index i as a tuple (sentence, label),\n","        where the sentence is tokenized.\n","        \"\"\"\n","        encoded = self.tokenizer.encode(\n","            self.sentences[idx], add_special_tokens=False)\n","        return encoded, self.labels[idx]\n","\n","# Now, we can load our train and validation data as Pytorch Dataset.\n","train_data = AFFRDataset(reviews_train, scores_train, tokenizer)\n","valid_data = AFFRDataset(reviews_valid, scores_valid, tokenizer)\n","\n","num_labels = len(set(train_data.labels))\n","print('num train/valid: {}/{}'.format(len(train_data), len(valid_data)))\n","print('num labels:', num_labels)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e56D8SLwGSB7","colab_type":"text"},"source":["**Step 4b**\n","\n","Next we need a DataLoader that takes care of batching and shuffling the data, this functionality is already present in the base DataLoader class. However, because sentences are not all the same length, we need to write a custom `collate_fn` that handles padding."]},{"cell_type":"code","metadata":{"id":"7AtbLE1uUJGF","colab_type":"code","colab":{}},"source":["from torch.utils.data import DataLoader\n","\n","def padded_collate(batch):\n","    \"\"\"Pad sentences, return sentences and labels as LongTensors.\"\"\"\n","    sentences, labels = zip(*batch)\n","    lengths = [len(s) for s in sentences]\n","    max_length = max(lengths)\n","    # Pad each sentence with zeros to max_length\n","    padded = [s + [0] * (max_length - len(s)) for s in sentences]\n","    return torch.LongTensor(padded), torch.LongTensor(labels)\n","\n","train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=padded_collate)\n","valid_loader = DataLoader(valid_data, batch_size=256, shuffle=False, collate_fn=padded_collate)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eH1GV_XoH8sr","colab_type":"text"},"source":["**Step 5**\n","\n","I've given you a very simple Bag-of-words classifier to see if everything works.\n","To train your model at a decent speed you want to use Colabs GPU capabilities. You can switch to a GPU session in `Edit -> Notebook settings -> Hardware Accelerator`"]},{"cell_type":"code","metadata":{"id":"qYfAiD_iWPB7","colab_type":"code","colab":{}},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from tqdm.notebook import tqdm\n","\n","class BOWClassifier(nn.Module):\n","    \"\"\"\n","    A basic Bag-of-words Classifier\n","    \"\"\"\n","    def __init__(self, vocab_size, output_size):\n","        super().__init__()\n","        self.emb = nn.EmbeddingBag(vocab_size, 64)\n","        self.fc_out = nn.Linear(64, output_size)\n","        \n","    def forward(self, x, y):\n","        h = self.emb(x)\n","        out = self.fc_out(h)\n","        loss = F.cross_entropy(out, y)\n","        return loss, out\n","\n","\n","def accuracy(logits, y):\n","    \"\"\"Calculate accuracy of a batch.\"\"\"\n","    pred = torch.softmax(logits, -1).argmax(-1)\n","    return (pred == y).float().mean()\n","\n","\n","def train_epoch(model, optimizer, data_loader, device):\n","    \"\"\"Train model for one epoch\"\"\"\n","    # tqdm notebook turns any iterable into a progress bar. \n","    for bx, by in tqdm(data_loader, leave=False):\n","        model.train()\n","        optimizer.zero_grad()\n","        loss, out = model(bx.to(device), by.to(device))\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","def validate(model, data_loader, device):\n","    \"\"\"Validate model\"\"\"\n","    model.eval()\n","    total_accuracy = 0\n","    total_loss = 0\n","    num_valid = 0\n","    for bx, by in data_loader:\n","        # Don't calculate gradients during validation.\n","        with torch.no_grad():\n","            loss, out = model(bx.to(device), by.to(device))\n","            b_accuracy = accuracy(out.detach(), by.to(device))\n","            # Save accuracy and loss as a sum instead of batch mean, \n","            # so we can take the mean of the total validation set later.\n","            total_accuracy += b_accuracy * out.size(0)\n","            total_loss += loss.detach().item() * out.size(0)\n","            num_valid += out.size(0)\n","    valid_loss = total_loss / num_valid\n","    valid_acc = total_accuracy / num_valid\n","    return valid_loss, valid_acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4m_V5Z_vWowV","colab_type":"code","colab":{}},"source":["# Train your model for a few epochs to see if everything works.\n","print(\"Using CUDA:\", torch.cuda.is_available())\n","num_epochs = 5\n","device = 'cuda'\n","model = BOWClassifier(tokenizer.vocab_size, 3).to(device)\n","opt = torch.optim.Adam(model.parameters())\n","\n","for epoch in range(num_epochs):\n","    train_epoch(model, opt, train_loader, device)\n","    valid_loss, valid_acc = validate(model, valid_loader, device)\n","    print(\"epoch {} loss {:.3f} accuracy {:.3f}\".format(epoch + 1, valid_loss, valid_acc))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KBXEsPgLKes1","colab_type":"text"},"source":["**Step 6a**\n","\n","Since a few versions Pytorch has basic Tensorboard operations built in. We will use this to visualise the training progress. First we need to install and load the TensorBoard notebook extension.\n","\n","*Note*: Tensorboard might already be installed on your Colab instance."]},{"cell_type":"code","metadata":{"id":"JXP9xVHNYgkH","colab_type":"code","colab":{}},"source":["!pip install tensorboard\n","%load_ext tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2G-BfJ2IMLCQ","colab_type":"text"},"source":["**Step 6b**\n","Modify the training code so it saves all desired values a Tensorboard SummaryWriter.\n","For some inspiration, see the pytorch tensorboard documentation: https://pytorch.org/docs/stable/tensorboard.html .\n","\n","You want to at least report the training loss, validation loss, train accuracy, and validation accuracy."]},{"cell_type":"code","metadata":{"id":"Apndg48BLcf_","colab_type":"code","colab":{}},"source":["def train_epoch(*args):\n","    \"\"\"Train model for one epoch and save to Tensorboard\"\"\"\n","    pass\n","\n","def validate(*args):\n","    \"\"\"Validate model and save to Tensorboard\"\"\"\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3QFj7jn1Mr2M","colab_type":"text"},"source":["**Step 6c**\n","\n","To view the training progress on Tensorboard, simply use the tensorboard Magic command. To both show the Tensorboard app and train at the same time, first launch tensorboard here, and then start training.\n","\n","*Note:* Running Tensorboard from a notebook might be impractical in some cases, you can also run Tensorboard locally on your machine if you sync the Tensorboard logs through Google Drive."]},{"cell_type":"code","metadata":{"id":"oe0WUbifNx6I","colab_type":"code","colab":{}},"source":["%tensorboard --logdir ./logs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"40TxHshPS3jp","colab_type":"text"},"source":["**Step 7**\n","\n","On Colab, you cannot use GPU instances infinitely. There is a maximum runtime of 12 hours, after which you will need to connect to a different VM. Most of the projects in this course will not need longer than 12 hours of training time, but it is always smart to save your model between epochs in case anything goes wrong during training.\n","\n","In addition to saving the model weights, saving the optimizer state and other statistics (such as number of training steps) is advised. [This Blogpost](https://medium.com/udacity-pytorch-challengers/saving-loading-your-model-in-pytorch-741b80daf3c) gives a good overview of all the options. The two functions below give a good starting point for simple models.\n","\n","Modify your training script in the previous cells to save the best model, and load the best model here to validate."]},{"cell_type":"code","metadata":{"id":"fq3AK5OfS2pj","colab_type":"code","colab":{}},"source":["def save_model(path, model, optimizer, step):\n","    \"\"\"\n","    Save model, optimizer and number of training steps to path.\n","    \"\"\"\n","    checkpoint = {'state_dict': model.state_dict(),\n","                  'optimizer': optimizer.state_dict(),\n","                  'step': step}\n","    torch.save(checkpoint, path)\n","\n","def load_model(path, model, optimizer, device):\n","    \"\"\"\n","    Load a model and optimizer state to device from path.\n","    \"\"\"\n","    checkpoint = torch.load(path, map_location=device)\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    return model, optimizer, checkpoint['step']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nBpqXB6IOMDY","colab_type":"text"},"source":["**Step 8** (Optional)\n","\n","Of course, the performance of a bag-of-words classifier is far from state of the art. As a final step design your own classifier and evaluate it on tensorboard. \n","\n","Get creative! Pytorch has implementations for RNNs, CNNs, and even Transformer architectures. A good classifier should be able to achieve at least 85% validation accuracy."]},{"cell_type":"code","metadata":{"id":"eJtB8zWfZUJv","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}