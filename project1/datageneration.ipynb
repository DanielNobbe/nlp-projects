{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lennertjansen/nlp2/blob/master/project1/datageneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhUTSCXSzOws",
        "colab_type": "text"
      },
      "source": [
        "Mount notebook to drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYoYPc-FzWeA",
        "colab_type": "code",
        "outputId": "921cf3a1-60e0-47c1-bbfd-ed4574d14232",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltKVMbGoUwcU",
        "colab_type": "code",
        "outputId": "dfe5892f-0164-44fa-d611-866be926d5eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%cd drive/My\\ Drive/nlp2/code\n",
        "!ls"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/My Drive/nlp2/code'\n",
            "/content/drive/.shortcut-targets-by-id/1QXwx2THcNLhcOFb0uOdUp12fL_lxsei0/nlp2/code\n",
            "'Colab Tutorial'   OpenNMT-py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWcmAuhNQCWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVtzYYRxVDC1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3fa35856-3a50-481b-99af-72a4d8090266"
      },
      "source": [
        "\"\"\"\n",
        " To generate our dataset, we need a few things:\n",
        " 1. Grammar rules (So a PCFG) with all grammar rules\n",
        " 2. Lexical rules (a vocabulary)\n",
        " 3. A generation mechanism, that probabilistically samples how to fill in each instance, and that can stop. Should also be able to give metadata to each datapoint \n",
        " 4. Potentially a function that checks if the grammar is correct (could also be done by hand)\n",
        " 5. A translation function, that converts each generated sample into its correct interpretation. This needs interpretation rules, and a lookup table\n",
        " Notes:\n",
        " - Use nested dict for lookup table\n",
        " \"\"\""
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n To generate our dataset, we need a few things:\\n 1. Grammar rules (So a PCFG) with all grammar rules\\n 2. Lexical rules (a vocabulary)\\n 3. A generation mechanism, that probabilistically samples how to fill in each instance, and that can stop. Should also be able to give metadata to each datapoint \\n 4. Potentially a function that checks if the grammar is correct (could also be done by hand)\\n 5. A translation function, that converts each generated sample into its correct interpretation. This needs interpretation rules, and a lookup table\\n Notes:\\n - Use nested dict for lookup table\\n '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co6l9-Qn5y9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PCFG_rule:\n",
        "  def __init__(self, lhs, rhs, prob):\n",
        "    self.lhs = lhs\n",
        "    self.rhs = rhs\n",
        "    self.prob = prob\n",
        "  \n",
        "  def __str__(self):\n",
        "    if isinstance(self.rhs, list):\n",
        "      return (\"({}) {} --> {}\".format( self.prob, ' '.join(self.lhs), ' '.join(self.rhs) ))\n",
        "    else:\n",
        "      return (\"({}) {} --> {}\".format( self.prob, ' '.join(self.lhs), self.rhs ))\n",
        "\n",
        "class PCFG:\n",
        "  def __init__(self):\n",
        "    self.rules = {}\n",
        "\n",
        "  def add(self, lhs, rhs, prob):\n",
        "    rule = PCFG_rule(lhs, rhs, prob)\n",
        "    if isinstance(lhs,list):\n",
        "      lhs = tuple(lhs) # Tuple is hashable\n",
        "    if lhs in self.rules.keys():\n",
        "      self.rules[lhs].append(rule)\n",
        "    else:\n",
        "      self.rules[lhs] = [rule]\n",
        "\n",
        "  def select(self, lhs):\n",
        "    lhs = tuple(lhs)\n",
        "    return self.rules[lhs]\n",
        "\n",
        "  def sample(self, lhs):\n",
        "    # print(lhs, len(lhs))\n",
        "    assert not ((isinstance(lhs, list) or isinstance(lhs, tuple)) and len(lhs)>1), \"Only single words or non-terminals can be filled in.\"\n",
        "    if isinstance(lhs, list):\n",
        "      lhs = tuple(lhs)\n",
        "    # print(\"Sampling for \", lhs)\n",
        "    if not self.rules.get(lhs):\n",
        "      return lhs, True\n",
        "    rules = self.rules.get(lhs)\n",
        "    # print(rules)\n",
        "    probs = [rule.prob for rule in rules]\n",
        "    # print(\"probabilities: \", probs)\n",
        "    number_of_options = len(rules)\n",
        "\n",
        "    choice_index = np.random.choice(a = number_of_options, p = probs)\n",
        "    # print(\"Selected option: \", rules[choice_index].rhs)\n",
        "    return rules[choice_index].rhs, False\n",
        "\n",
        "  def generate(self, sequence):\n",
        "    # print(\"Generating for starting sequence \", sequence)\n",
        "    finished = False\n",
        "    while not finished:\n",
        "      finished_list = [False]*len(sequence)\n",
        "      new_sequence = []\n",
        "      for i, lhs in enumerate(sequence):\n",
        "        rhs, finished_list[i] = ruleset.sample(lhs)\n",
        "        if isinstance(rhs, list):\n",
        "          new_sequence.extend(rhs)\n",
        "        elif isinstance(rhs, str):\n",
        "          new_sequence.append(rhs)\n",
        "      sequence = new_sequence\n",
        "      # print(\"New sequence: \", sequence)\n",
        "      if all(finished_list):\n",
        "        finished = True\n",
        "    return sequence\n",
        "      \n",
        "\n",
        "\n",
        "  def __str__(self):\n",
        "    string = ''\n",
        "    for key in self.rules.keys():\n",
        "      for item in self.rules[key]:\n",
        "        # print(item)\n",
        "        string += item.__str__()\n",
        "        # print(item.__str__())\n",
        "        string += '\\n'\n",
        "    return string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L_twi8T4Im0",
        "colab_type": "code",
        "outputId": "8171d1c7-2b86-4c12-d0a8-153b8811a917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# 1: grammar rules. We will encode this in an array? dict? tuple? \n",
        "# dict - tuple: prob (where tuple - [lhs , rhs] for lhs --> rhs)\n",
        "# Better option: python array, with [lhs, rhs, prob] for each cfg rule\n",
        "# Where lhs is a list of each character in lhs, and rhs same for rhs\n",
        "# Actually, best option is to use a dict. Since we will be generating, the lhs should be the key.\n",
        "# So: dict of {lhs: [[rhs1, prob1], [rhs2, prob2]]}\n",
        "# rule = PCFG_rule()\n",
        "ruleset = PCFG()\n",
        "\n",
        "# Non-terminal rules:\n",
        "\n",
        "lhs = 'S' # valid sequence is an object: S --> O\n",
        "rhs = ['A', 'O']\n",
        "prob = 1\n",
        "ruleset.add(lhs, rhs, prob)\n",
        "\n",
        "lhs = 'O' # O --> A O\n",
        "rhs = ['A', 'O']\n",
        "prob = 0.15\n",
        "ruleset.add(lhs, rhs, prob)\n",
        "\n",
        "lhs = 'O' # O --> A T O\n",
        "rhs = ['A', 'T', 'O']\n",
        "prob = 0.05\n",
        "ruleset.add(lhs, rhs, prob)\n",
        "\n",
        "lhs = 'O' # O --> N O\n",
        "rhs = ['N', 'O']\n",
        "prob = 0.15\n",
        "ruleset.add(lhs, rhs, prob)\n",
        "\n",
        "# lhs = 'A' # A --> A N\n",
        "\n",
        "lhs = 'O' # O --> O & O\n",
        "rhs = ['O', '&', 'O']\n",
        "prob = 0.2\n",
        "ruleset.add(lhs, rhs, prob)\n",
        "\n",
        "\n",
        "\n",
        "lhs = 'T' # T --> W H\n",
        "rhs = ['W', 'H']\n",
        "prob = 1\n",
        "ruleset.add(lhs, rhs, prob)\n",
        "\n",
        "# lhs = ['A', 'O'] # A O --> A O & O\n",
        "# rhs = ['A', 'O', '&', 'O']\n",
        "# prob = 0.4\n",
        "# ruleset.add(lhs, rhs, prob)\n",
        "\n",
        "\n",
        "print('All non-terminal rules: \\n', ruleset)\n",
        "\n",
        "# Problem:\n",
        "# cut tomato and cut steak should be a valid S\n",
        "# S --> A O & A O \n",
        "# These should also be possibly nested in another action, so\n",
        "# A O --> A A O & A O\n",
        "# This reduces to O --> A O & A O, which reduces to O --> O & O \n",
        "# Above is true since O --> A O\n",
        "# O --> O & O should not be possible (why not?)\n",
        "# This causes line 47 to be ambiguous - unless we use a 'then' word in addition to and\n",
        "# 'then' would indicate the end of the scope of an action, where 'and' would include the next object"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All non-terminal rules: \n",
            " (1) S --> A O\n",
            "(0.15) O --> A O\n",
            "(0.05) O --> A T O\n",
            "(0.15) O --> N O\n",
            "(0.2) O --> O & O\n",
            "(1) T --> W H\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6V5z0pV7iQd",
        "colab_type": "code",
        "outputId": "1ad60be6-0135-4b6e-bd97-0bf87809f09a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "# All lexical rules:\n",
        "\n",
        "# Object rules:\n",
        "lhs = 'O'\n",
        "objects = ['tomato', 'steak', 'onion', 'potato', 'chicken', 'pork']\n",
        "obj_probs = [0.1, 0.1, 0.1, 0.05, 0.05, 0.05] # We don't like chicken and pork\n",
        "# Note: will the eventual model be largely carnivorous or herbivorous?\n",
        "for rhs, prob in zip(objects, obj_probs):\n",
        "  ruleset.add(lhs, rhs, prob)\n",
        "# print(ruleset)\n",
        "\n",
        "# Action rules:\n",
        "lhs = 'A'\n",
        "actions = ['cut', 'fry', 'grill', 'clean', 'boil']\n",
        "act_probs = [0.3, 0.2, 0.2, 0.2, 0.1]\n",
        "for rhs, prob in zip(actions, act_probs):\n",
        "  ruleset.add(lhs, rhs, prob)\n",
        "\n",
        "# Number rules:\n",
        "lhs = 'N'\n",
        "numbers = ['one', 'two', 'three', 'four', 'five']\n",
        "num_probs = [0.4, 0.3, 0.1, 0.1, 0.1]\n",
        "for rhs, prob in zip(numbers, num_probs):\n",
        "  ruleset.add(lhs, rhs, prob)\n",
        "\n",
        "# Connective rules:\n",
        "lhs = '&'\n",
        "connectives = ['and', 'after', 'then']\n",
        "conn_probs = [0.4, 0.2, 0.4]\n",
        "for rhs, prob in zip(connectives, conn_probs):\n",
        "  ruleset.add(lhs, rhs, prob)\n",
        "\n",
        "# With rule:\n",
        "lhs = 'W'\n",
        "w = ['with']\n",
        "w_prob = 1\n",
        "ruleset.add(lhs, w, w_prob)\n",
        "\n",
        "# Tool rules:\n",
        "# lhs = ['H']\n",
        "# tools = ['vegknife', 'meatknife', 'fryingpan', 'grill', 'boil', 'season']\n",
        "# conn_probs = [0., 0.2, 0.4]\n",
        "# for rhs, prob in zip(connectives, conn_probs):\n",
        "#   ruleset.add(lhs, rhs, prob)\n",
        "# How do we make sure these only appear in the correct context?\n",
        "# Probably easiest to filter the wrong ones out?\n",
        "# Or use context based rules for generation. Or even substitute actions as 'action with tool'\n",
        "# strings? Probably context based rules is best, based on intuition.\n",
        "# Probably we should implement the tools in a different function than in the PCFG\n",
        "\n",
        "print(ruleset)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1) S --> A O\n",
            "(0.15) O --> A O\n",
            "(0.05) O --> A T O\n",
            "(0.15) O --> N O\n",
            "(0.2) O --> O & O\n",
            "(0.1) O --> tomato\n",
            "(0.1) O --> steak\n",
            "(0.1) O --> onion\n",
            "(0.05) O --> potato\n",
            "(0.05) O --> chicken\n",
            "(0.05) O --> pork\n",
            "(1) T --> W H\n",
            "(0.3) A --> cut\n",
            "(0.2) A --> fry\n",
            "(0.2) A --> grill\n",
            "(0.2) A --> clean\n",
            "(0.1) A --> boil\n",
            "(0.4) N --> one\n",
            "(0.3) N --> two\n",
            "(0.1) N --> three\n",
            "(0.1) N --> four\n",
            "(0.1) N --> five\n",
            "(0.4) & --> and\n",
            "(0.2) & --> after\n",
            "(0.4) & --> then\n",
            "(1) W --> with\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KRfnL0lGfNL",
        "colab_type": "code",
        "outputId": "f0b630b7-e3f6-4edb-8b19-c5c8f71c2c06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "# Now, part two: generation\n",
        "# We will generate grammatically correct sequences, where we only leave the tools not filled in.\n",
        "# We always start from an S, and then randomly pick a way to fill it in, recursively\n",
        "sequences = []\n",
        "for i in range(20):\n",
        "  sequence = ['S']\n",
        "  sequence = ruleset.generate(sequence)\n",
        "  sequences.append(sequence)\n",
        "  print(\"Sequence {}: {}\".format(i,sequence))\n",
        "\n",
        "\"\"\" Observations 8/04:\n",
        "Sequence 0: ['cut', 'three', 'onion']\n",
        "Sequence 1: ['fry', 'fry', 'with', 'H', 'clean', 'five', 'pork', 'then', 'tomato']\n",
        "Sequence 2: ['cut', 'steak']\n",
        "Sequence 3: ['clean', 'tomato', 'and', 'clean', 'four', 'tomato']\n",
        "Sequence 4: ['fry', 'pork', 'and', 'steak', 'and', 'clean', 'one', 'one', 'cut', 'tomato', 'and', 'fry', 'pork']\n",
        "Sequence 5: ['grill', 'steak', 'and', 'two', 'tomato', 'and', 'onion', 'after', 'chicken']\n",
        "Sequence 6: ['clean', 'pork']\n",
        "Sequence 7: ['fry', 'potato']\n",
        "Sequence 8: ['cut', 'potato']\n",
        "Sequence 9: ['cut', 'potato']\n",
        "Sequence 10: ['cut', 'fry', 'fry', 'pork']\n",
        "Sequence 11: ['fry', 'potato']\n",
        "Sequence 12: ['grill', 'two', 'potato', 'then', 'tomato']\n",
        "Sequence 13: ['clean', 'potato']\n",
        "Sequence 14: ['grill', 'five', 'one', 'tomato']\n",
        "Sequence 15: ['fry', 'steak', 'then', 'pork']\n",
        "Sequence 16: ['grill', 'fry', 'fry', 'potato', 'and', 'pork', 'then', 'fry', 'with', 'H', 'potato', 'then', 'steak']\n",
        "Sequence 17: ['cut', 'steak']\n",
        "Sequence 18: ['boil', 'grill', 'pork', 'then', 'tomato']\n",
        "Sequence 19: ['fry', 'tomato', 'and', 'cut', 'one', 'potato', 'then', 'onion']\n",
        "\n",
        "We see sometimes a double number. This is kind of weird for human language, but should be OK in our task.\n",
        "Also we see single objects, like sequence 18. This should be interpreted as calling the action (boil grill) on both objects\n",
        "\"\"\""
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence 0: ['cut', 'steak']\n",
            "Sequence 1: ['fry', 'clean', 'with', 'H', 'potato']\n",
            "Sequence 2: ['cut', 'two', 'one', 'steak', 'then', 'steak', 'and', 'pork']\n",
            "Sequence 3: ['fry', 'tomato']\n",
            "Sequence 4: ['cut', 'four', 'two', 'one', 'fry', 'tomato', 'after', 'grill', 'with', 'H', 'pork']\n",
            "Sequence 5: ['grill', 'tomato', 'then', 'steak']\n",
            "Sequence 6: ['boil', 'onion']\n",
            "Sequence 7: ['clean', 'one', 'tomato']\n",
            "Sequence 8: ['cut', 'onion']\n",
            "Sequence 9: ['boil', 'onion', 'after', 'clean', 'with', 'H', 'boil', 'with', 'H', 'potato']\n",
            "Sequence 10: ['clean', 'onion', 'then', 'four', 'onion', 'after', 'tomato', 'after', 'clean', 'steak']\n",
            "Sequence 11: ['clean', 'clean', 'with', 'H', 'two', 'boil', 'with', 'H', 'chicken']\n",
            "Sequence 12: ['cut', 'four', 'potato', 'and', 'grill', 'tomato', 'and', 'two', 'one', 'two', 'cut', 'tomato', 'then', 'tomato']\n",
            "Sequence 13: ['clean', 'chicken', 'then', 'onion']\n",
            "Sequence 14: ['cut', 'grill', 'pork', 'then', 'potato']\n",
            "Sequence 15: ['clean', 'onion']\n",
            "Sequence 16: ['fry', 'steak']\n",
            "Sequence 17: ['cut', 'chicken']\n",
            "Sequence 18: ['grill', 'steak']\n",
            "Sequence 19: ['boil', 'steak']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Observations 8/04:\\nSequence 0: ['cut', 'three', 'onion']\\nSequence 1: ['fry', 'fry', 'with', 'H', 'clean', 'five', 'pork', 'then', 'tomato']\\nSequence 2: ['cut', 'steak']\\nSequence 3: ['clean', 'tomato', 'and', 'clean', 'four', 'tomato']\\nSequence 4: ['fry', 'pork', 'and', 'steak', 'and', 'clean', 'one', 'one', 'cut', 'tomato', 'and', 'fry', 'pork']\\nSequence 5: ['grill', 'steak', 'and', 'two', 'tomato', 'and', 'onion', 'after', 'chicken']\\nSequence 6: ['clean', 'pork']\\nSequence 7: ['fry', 'potato']\\nSequence 8: ['cut', 'potato']\\nSequence 9: ['cut', 'potato']\\nSequence 10: ['cut', 'fry', 'fry', 'pork']\\nSequence 11: ['fry', 'potato']\\nSequence 12: ['grill', 'two', 'potato', 'then', 'tomato']\\nSequence 13: ['clean', 'potato']\\nSequence 14: ['grill', 'five', 'one', 'tomato']\\nSequence 15: ['fry', 'steak', 'then', 'pork']\\nSequence 16: ['grill', 'fry', 'fry', 'potato', 'and', 'pork', 'then', 'fry', 'with', 'H', 'potato', 'then', 'steak']\\nSequence 17: ['cut', 'steak']\\nSequence 18: ['boil', 'grill', 'pork', 'then', 'tomato']\\nSequence 19: ['fry', 'tomato', 'and', 'cut', 'one', 'potato', 'then', 'onion']\\n\\nWe see sometimes a double number. This is kind of weird for human language, but should be OK in our task.\\nAlso we see single objects, like sequence 18. This should be interpreted as calling the action (boil grill) on both objects\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQtNcHXOQYB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In this cell, we define the lookup table for the tools pertaining to (object, action) pairs. We do this through nested dicts\n",
        "tools = {\n",
        "  'cut': {\n",
        "        'onion': 'veg-knife',\n",
        "        'tomato': 'veg-knife',\n",
        "        'steak': 'meat-knife',\n",
        "        'potato': 'veg-knife',\n",
        "        'chicken': 'chicken-knife',\n",
        "        'pork': 'meat-knife'\n",
        "    },\n",
        "  'fry': {\n",
        "      'onion': 'frying-pan',\n",
        "      'tomato': 'frying-pan',\n",
        "      'steak': 'frying-pan',\n",
        "      'potato': 'frying-pan',\n",
        "      'chicken': 'frying-pan',\n",
        "      'pork': 'frying-pan'\n",
        "  },\n",
        "  'grill': {\n",
        "      'onion': 'griddle',\n",
        "      'tomato': 'griddle',\n",
        "      'steak': 'skillet',\n",
        "      'potato': 'skillet',\n",
        "      'chicken': 'griddle',\n",
        "      'pork': 'skillet'\n",
        "  },\n",
        "  'clean': {\n",
        "      'onion': 'peeler',\n",
        "      'tomato': 'water',\n",
        "      'steak': 'fillet-knife',\n",
        "      'potato': 'peeler',\n",
        "      'chicken': 'fillet-knife',\n",
        "      'pork': 'fillet-knife'\n",
        "  },\n",
        "  'boil': {\n",
        "      'onion': 'soup-pot',\n",
        "      'tomato': 'soup-pot',\n",
        "      'steak': 'stew-pot',\n",
        "      'potato': 'stew-pot',\n",
        "      'chicken': 'soup-pot',\n",
        "      'pork': 'stew-pot'\n",
        "  },\n",
        "  'season': {\n",
        "      'onion': 'salt',\n",
        "      'tomato': 'pepper',\n",
        "      'steak': 'rosemary',\n",
        "      'potato': 'salt',\n",
        "      'chicken': 'pepper',\n",
        "      'pork': 'pepper'\n",
        "  },\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfCVBaJwOip4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "02596ca7-60db-43c3-a7df-e34763de9046"
      },
      "source": [
        "\"\"\" Next, we need to fill in the right tools. This uses some context. \n",
        " We should loop through each sentence, and wherever there is an open H, fill it in according to the action and next object\n",
        " Problem: some actions have tools assigned but operate on multiple objects\n",
        " --> actions with tools should operate only on single objects.\n",
        " Solution: remove everything until nesting ends for those sentences - so until the & before next action\n",
        " - This should actually be encoded in the context free grammar. However, this requires two terms in the lhs, so we circumvent that by filtering like this\n",
        " \"\"\"\n",
        "\n",
        "action_found = False\n",
        "H_found = False\n",
        "with_found = False\n",
        "remove = False\n",
        "with_index = 0\n",
        "remove_ending = 0\n",
        "for seq_index, seq in enumerate(sequences):\n",
        "  # We loop through all sequences. Seq is now a list of words, with sometimes an H.\n",
        "  for token_index, token in enumerate(seq):\n",
        "    if token in actions:\n",
        "      action = token\n",
        "      action_found = True\n",
        "      continue\n",
        "    if not ((action_found and (token == 'with')) or with_found):\n",
        "      # print(\"With not found\")\n",
        "      action_found = False\n",
        "    elif token == 'with':\n",
        "      with_found = True\n",
        "      with_index = token_index\n",
        "      # print(\"With found!\", action_found, with_found)\n",
        "      continue\n",
        "    # if (action_found and with_found):\n",
        "    #   print(\"token after with found and action found: \", token)\n",
        "    if action_found and with_found and (token == 'H'):\n",
        "      # print(\"H found\")\n",
        "      H_index = token_index\n",
        "      H_found = True\n",
        "      continue\n",
        "    if action_found and H_found and (token in objects):\n",
        "      print(\"Found token after H and action in sequence: \\n\", seq)\n",
        "      obj = token\n",
        "      print(\"object: \", obj)\n",
        "      print(\"Action: \", action) # these can be used to look up what's needed. Then we remove the tokens up to the next action.\n",
        "      tool = tools[action][obj]\n",
        "      print(\"Tool:\", tool)\n",
        "      seq[H_index] = tool\n",
        "      remove = True\n",
        "      remove_start = token_index+1\n",
        "      rest_of_sequence = seq[remove_start:]\n",
        "      print(seq[remove_start:])\n",
        "      remove_end = [index-1 for index, token in enumerate(rest_of_sequence) if (token in actions and rest_of_sequence[index-1] in connectives) ]\n",
        "      if len(remove_end)>0:\n",
        "        remove_ending = remove_end\n",
        "      print(remove_ending)\n",
        "      # print(\"a: \", seq[:remove_start])\n",
        "      # print(\"b: \", seq[remove_start+remove_ending:])\n",
        "      sequences[seq_index] = seq[:remove_start].extend(seq[remove_start+remove_ending:])\n",
        "\n",
        "      action_found = False\n",
        "      H_found = False\n",
        "      # Note that this only deals with a single H in a sequence. If there are more, we need to repeat. Can we call a function recursively to fix this?\n",
        "      # print(sequence)\n",
        "\n",
        "      \n",
        "\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found token after H and action in sequence: \n",
            " ['clean', 'clean', 'with', 'H', 'two', 'boil', 'with', 'H', 'chicken']\n",
            "object:  chicken\n",
            "Action:  boil\n",
            "Tool: soup-pot\n",
            "[]\n",
            "0\n",
            "a:  ['clean', 'clean', 'with', 'H', 'two', 'boil', 'with', 'soup-pot', 'chicken']\n",
            "b:  []\n",
            "['boil', 'steak']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37P5lDAzLTE8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd7243e3-935c-4960-8d50-4fe09fd689f0"
      },
      "source": [
        "dicti = {'a': 2, 'b': 3}\n",
        "print(dicti['a'])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auFMRG1PRJtt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}